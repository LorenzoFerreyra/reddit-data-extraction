---
title: "Trabajo de Taller de Investigación: Text mining"
author: "Lorenzo Ferreyra"
format: html
editor: visual
---

## Librerías

Importamos las librerías necesarias.

```{r}
#| echo: false
library(readxl)
library(dplyr)
library(stringr)
library(ggplot2)
library(knitr)
library(tidytext)
library(stopwords)
```

## Carga de datos

Se leen los archivos de Excel. En el paso anterior de transformación, se dividió el archivo .json extraído de Reddit en dos entidades: comentarios y publicaciones, para mantener una estructura relacional y que los comentarios no pierdan los metadatos de las publicaciones asociadas.

```{r}
comments <- read_excel("output_files/comments.xlsx")
posts <- read_excel("output_files/posts.xlsx")
```

Se obtiene un vistazo preliminar de los primeros registros de los conjuntos de datos para verificar que la lectura del archivo fue exitosa y entender la estructura de las variables antes de iniciar el análisis como tal.

### Estructura de comentarios

```{r}
head(comments, 5) %>% 
  kable()
```

### Estructura de publicaciones

```{r}
head(posts, 5) %>% 
  kable()
```

## Preprocesamiento de datos

Se limpia el texto de los comentarios; se pasa todo a minúsculas, se eliminan links, símbolos irrelevantes, etc.

```{r}
comments_clean <- comments %>%
  mutate(body = str_to_lower(body)) %>%
  mutate(body = str_remove_all(body, "http\\S+|www\\S+")) %>%
  mutate(body = str_remove_all(body, "[^a-záéíóúüñ\\s]")) %>%
  mutate(body = str_squish(body))
```

Observación del dataframe luego del procesamiento de datos.

```{r}
head(comments_clean, 5) %>% 
  kable()
```

## Tokenización

```{r}

comentarios_tokenizados <- comments_clean %>%
  unnest_tokens(palabra, body)
head(comentarios_tokenizados, 5) %>% 
  kable()

```

## Eliminación de stopwords

```{r}

stopwords_es <- tibble(palabra = stopwords("es"))
```

## Personalización de las stopwords

Dado que las stopwords que nos ofrece la librería stopwords para la lengua española no es exhaustiva, se la personaliza para agregar unidades relevantes para este caso.

```{r}
mis_stopwords <- tibble(palabra = c("si", "mas", "va", "hace","ahora", "trabajo", "puede", "tener", "menos", "solo", "vos",
"mismo", "van", "ver", "mejor", "creo", "sino", "sos", "q","así"))
stopwords_es <- tibble(palabra = stopwords("es")) %>%
  bind_rows(mis_stopwords)
comentarios_filtrados <- comentarios_tokenizados %>%
  anti_join(stopwords_es, by = "palabra")
```

## Análisis de frecuencia de palabras

```{r}
frecuencias <- comentarios_filtrados %>%
  count(palabra, sort = TRUE)

head(frecuencias, 20) %>%
  ggplot(aes(x = reorder(palabra, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Palabras más frecuentes en los comentarios", x = "", y = "Frecuencia")

```

Palabras como ia, código, gente, programadores y laburo, además de años, demuestran que en el conjunto de datos se encuentran discusiones relevantes no solo sobre la inteligencia artificial generativa como tal, sino sobre el futuro del trabajo y la productividad.

## Análisis de sentimientos

```{r}
sentimientos <- get_sentiments("nrc", language = "spanish")

sentimiento_comentarios <- comentarios_filtrados %>%
  inner_join(sentimientos, by = c("palabra" = "word")) %>%
  count(sentiment, sort = TRUE)

ggplot(sentimiento_comentarios, aes(x = reorder(sentiment, n), y = n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(title = "Distribución de sentimientos en los comentarios")

```
