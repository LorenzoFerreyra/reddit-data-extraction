---
title: "Trabajo de Taller de investigación: text mining"
author: "Lorenzo Ferreyra"
format: 
  html:
    self-contained: true
editor: visual
---

## Librerías

Importamos las librerías necesarias.

```{r}
#| echo: false
library(readxl)
library(dplyr)
library(stringr)
library(ggplot2)
library(knitr)
library(tidytext)
library(stopwords)
library(wordcloud2)
library(syuzhet)
library(tidyr)
library(udpipe)
library(tm)
library(reshape2)
library(topicmodels) 
```

## Carga de datos

Se leen los archivos de Excel. En el paso anterior de transformación, se dividió el archivo .json extraído de Reddit en dos entidades: comentarios y publicaciones, para mantener una estructura relacional y que los comentarios no pierdan los metadatos de las publicaciones asociadas.

```{r}
comments <- read_excel("output_files/comments.xlsx")
posts <- read_excel("output_files/posts.xlsx")
```

Se obtiene un vistazo preliminar de los primeros registros de los conjuntos de datos para verificar que la lectura del archivo fue exitosa y entender la estructura de las variables antes de iniciar el análisis como tal.

### Estructura de comentarios

```{r}
head(comments, 5) %>% 
  kable()
```

### Estructura de publicaciones

```{r}
head(posts, 5) %>% 
  kable()
```

## Preprocesamiento de datos

Se limpia el texto de los comentarios; se pasa todo a minúsculas, se eliminan links, símbolos irrelevantes, etc.

```{r}
comments_clean <- comments %>%
  mutate(body = str_to_lower(body)) %>%
  mutate(body = str_remove_all(body, "http\\S+|www\\S+")) %>%
  mutate(body = str_remove_all(body, "[^a-záéíóúüñ\\s]")) %>%
  mutate(body = str_squish(body))
```

Como los títulos y el cuerpo de las publicaciones también tienen información muy relevantes, se hará una combinación de ambos dataframes, con el mismo procesamiento.

```{r}
posts_clean <- posts %>%
  mutate(body = str_to_lower(body)) %>%
  mutate(body = str_remove_all(body, "http\\S+|www\\S+")) %>%
  mutate(body = str_remove_all(body, "[^a-záéíóúüñ\\s]")) %>%
  mutate(body = str_squish(body))

```

El resultado del preprocesamiento son los siguientes dataframes. Publicaciones:

```{r}
head(posts_clean, 100) %>% 
  kable()
```

Comentarios:

```{r}
head(comments_clean, 5) %>% 
  kable()
```

## Lematización y tokenización

Se lematizan, tokenizan y combinan ambos dataframes con rbind.

```{r}
modelo_ud <- udpipe_download_model(language = "spanish", model_dir = "models")
modelo <- udpipe_load_model("models/spanish-gsd-ud-2.5-191206.udpipe")
anotados_comentarios <- udpipe_annotate(modelo, x = comments_clean$body)
anotados_comentarios <- as.data.frame(anotados_comentarios)
anotados_comentarios$origen <- "comentario"
anotados_posts <- udpipe_annotate(modelo, x = posts_clean$body)
anotados_posts <- as.data.frame(anotados_posts)
anotados_posts$origen <- "post"
tokens_combinados <- bind_rows(anotados_comentarios, anotados_posts)


```

Observación del dataframe luego del procesamiento de datos y la lematización (que incluye el tokenizado).

```{r}

head(tokens_combinados %>% select(token, lemma, upos, origen), 10) %>% 
  kable()
```

## Eliminación de stopwords

```{r}

stopwords_es <- tibble(palabra = stopwords("es"))
```

## Personalización de las stopwords

Dado que las stopwords que nos ofrece la librería stopwords para la lengua española no es exhaustiva, se la personaliza para agregar unidades relevantes para este caso.

```{r}
mis_stopwords <- tibble(lemma = c("si", "mas", "ahora", "menos", "solo", "vos",
"mismo", "mejor", "sino", "q","así", "ser", "hacer",  "tener", "haber", "poder", "ir", "decir", "usar", "saber", "dar", "pasar", "querer", "ver", "creer"))
stopwords_es <- tibble(lemma = stopwords("es")) %>%
  bind_rows(mis_stopwords)
tokens_combinados <- tokens_combinados %>%
  filter(!is.na(lemma)) %>%
  anti_join(stopwords_es, by = "lemma")
```

### Análisis de frecuencia de palabras

```{r}
frecuencias <- tokens_combinados %>%
  count(lemma, sort = TRUE)

head(frecuencias, 20) %>%
  ggplot(aes(x = reorder(lemma, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Palabras más frecuentes en los comentarios", x = "", y = "Frecuencia")

```

```{r}
wordcloud2(data = frecuencias, size = 1.5, color = 'random-light', backgroundColor = "black")

```

Palabras como ia, código, gente, programadores y laburo, además de años, demuestran que en el conjunto de datos se encuentran discusiones relevantes no solo sobre la inteligencia artificial generativa como tal, sino sobre el futuro del trabajo y la productividad.

## Análisis de sentimientos

```{r}
sentimientos <- get_nrc_sentiment(tokens_combinados$lemma, language = "spanish")

```

```{r}
sentimientos$palabra <- tokens_combinados$lemma

```

```{r}
sentimientos_totales <- sentimientos %>%
  select(-palabra) %>%  # Quitamos la columna palabra para el resumen
  summarise(across(everything(), sum)) %>%  # Sumamos por columna
  pivot_longer(cols = everything(), names_to = "sentimiento", values_to = "frecuencia") %>%
  arrange(desc(frecuencia))
ggplot(sentimientos_totales, aes(x = reorder(sentimiento, frecuencia), y = frecuencia, fill = sentimiento)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Distribución de sentimientos en los comentarios",
    x = "Sentimiento",
    y = "Frecuencia"
  )
```

A partir del análisis de los comentarios y publicaciones combinados, se observa una predominancia de sentimientos positivos, seguidos por negativos, y en menor medida, emociones como *trust*, *anticipation*, *sadness* y *fear*. Esta distribución posibilta la hipótesis de una actitud pragmática y funcional por parte de los profesionales IT frente a la inteligencia artificial generativa, con una valoración positiva de su potencial productivo, aunque sin dejar de lado preocupaciones técnicas o éticas.

Asimismo, el hecho de que las emociones negativas no sean dominantes, pero sí estén presentes en forma significativa (*fear*, *sadness*, *anger*), permite identificar tensiones latentes respecto al impacto de estas tecnologías, especialmente cuando se piensa en la calidad de los resultados y el futuro del trabajo.

```{r}
polaridad <- sentimientos_totales %>%
  filter(sentimiento %in% c("positive", "negative"))

ggplot(polaridad, aes(x = sentimiento, y = frecuencia, fill = sentimiento)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Polaridad de los sentimientos", x = "Polaridad", y = "Frecuencia")

```

## Interpretación de resultados y discusión

### ¿Qué temas o preocupaciones se mencionan con mayor frecuencia?

```{r}
tokens_combinados <- tokens_combinados %>%
  mutate(
    menciona_productividad = if_else(str_detect(lemma, 
      "automatizar|automatización|ayuda|mejora|productividad|eficiencia|rápido|velocidad|optimizar|optimización|rendimiento|agilizar|facilitar|acelerar|aumentar"), 1, 0),
    
    menciona_empleo = if_else(str_detect(lemma, 
      "desempleo|reemplazo|trabajo|empleo|ocupación|puesto|laboral|desarrollador|profesión|oficio|recursos|sustituir|desplazar|perder"), 1, 0),
    
    menciona_errores = if_else(str_detect(lemma, 
      "alucinar|alucinación|error|equivocar|confundir|malo|incorrecto|fallo|problema|falla|defecto|limitación|inexacto|bug"), 1, 0),
    
    menciona_etica = if_else(str_detect(lemma, 
      "sesgo|ética|desinformación|manipular|engaño|falsedad|engañoso|responsabilidad|transparencia|discriminación|prejuicio|moral|control|censura|justicia|privacidad"), 1, 0)
  )
menciones_totales <- tokens_combinados %>%
  summarise(
    productividad = sum(menciona_productividad),
    empleo = sum(menciona_empleo),
    errores = sum(menciona_errores),
    etica = sum(menciona_etica)
  ) %>%
  pivot_longer(cols = everything(), names_to = "categoria", values_to = "frecuencia")

ggplot(menciones_totales, aes(x = reorder(categoria, frecuencia), y = frecuencia, fill = categoria)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Frecuencia de menciones por tema clave",
    x = "Categoría",
    y = "Frecuencia"
  ) +
  scale_fill_brewer(palette = "Set2")

```

### ¿Qué actitudes predominan: entusiasmo, escepticismo, temor, indiferencia?

```{r}
{r}
actitudes <- sentimientos_totales %>%
mutate(
actitud = case_when(
sentimiento %in% c("joy", "trust", "positive", "anticipation") ~ "entusiasmo",
sentimiento %in% c("fear", "anger", "sadness", "disgust", "negative") ~ "temor / escepticismo",
TRUE ~ "otro"
)
) %>%
group_by(actitud) %>%
summarise(frecuencia = sum(frecuencia))

ggplot(actitudes, aes(x = actitud, y = frecuencia, fill = actitud)) +
geom_col(show.legend = FALSE) +
labs(title = "Actitudes predominantes en los comentarios", x = "", y = "Frecuencia")
```

```{r}

dtm <- tokens_combinados %>%
  count(doc_id, lemma) %>%
  cast_dtm(document = doc_id, term = lemma, value = n)

lda_model <- LDA(dtm, k = 4, control = list(seed = 1234))

```

Se extraen las palabras más representativas por tema. Se calcula la probabilidad de cada palabra en cada tópico. Luego, se agrupa por tópico y se eligen las 10 palabras más representativas (con mayor beta), ya que las palabras con mayor beta para cada tópico son las que mejor definen ese tema.

```{r}
topics <- tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta)
```

Aquí se agrupan las palabras por tópico (una faceta por cada tema) y se muestra qué tan importante (alta beta) es cada término para ese tópico.

```{r}
ggplot(topics, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Tópicos identificados por LDA", x = "Términos", y = "Importancia")
```
